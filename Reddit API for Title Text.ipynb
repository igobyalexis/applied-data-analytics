{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is intended to scrape any number of posts and the various components of those posts from Reddit.com. I use the PRAW io.wrapper to interact with the objects of the Reddit API.\n",
    "\n",
    "The goal of this particular script is to randomly scrape a subreddit, store the titles, number of comments, score, URL, unique identifier, and the date the post was created. \n",
    "\n",
    "Does the title of a post on a given subreddit on Reddit.com differ in the average number of words and the average word length, from subreddit to subreddit.\n",
    "\n",
    "Does the length of the title on a post in total characters have any correlation with the score received by the post?\n",
    "\n",
    "Further research to explore might include:\n",
    "1) Total karma of the user who submitted the post correlating with post performance (score)?\n",
    "\n",
    "Assumptions: \n",
    "1) The \"Best\" 10000 posts from each of the selected subreddits for \"all-time\" (aka \"best\")\n",
    "2) Taken from a single snapshot in time (11/13/2019) \n",
    "    - this should not be overly impacted due to sorting method by \"best\"\n",
    "3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup credentials OAuth2 to connect to reddit api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isfile\n",
    "import praw\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import urllib\n",
    "import os\n",
    "import io\n",
    "import datetime\n",
    "import linecache\n",
    "import glob\n",
    "\n",
    "import sqlite3\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "from bs4.element import Comment\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='6vtLxpH0AGMEJg',\n",
    "                     client_secret='RvdVqYTAPuZqapmtWq7nZNIdj30',\n",
    "                     password='Wardo1986',\n",
    "                     user_agent='ScrapeLifeTT by /u/Relevant_Telephone',\n",
    "                     username='Relevant_Telephone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant_Telephone\n"
     ]
    }
   ],
   "source": [
    "print(reddit.user.me())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open Recruitment Thread!\n",
      "[VIDEO] Clash of Clans World Finals 2019 - Official After Movie\n",
      "[Humor] Arnold is the GOAT üêê\n",
      "[GOAL] Finally hit max th10! (Ignore Michael, he's my pet cannon)\n",
      "[IDEAS] instead of skin on gold pass why should we not add background layout into our village that would be more cooler\n"
     ]
    }
   ],
   "source": [
    "# get any number of hot posts from the subreddit of your choosing but only the text of the title. \n",
    "hot_posts = reddit.subreddit('random').hot(limit=5)\n",
    "for post in hot_posts :\n",
    "    print(post.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block allows you to change how many posts you want to pull, whether you want it to be ranked by \"hotness\" or some other filter, and to input a specific subreddit, or run at random. This is put into a dataframe called \"posts\" - this list can now be iterated into a local file or to an instance of GBQ.\n",
    "Here are some of my favorite subreddits:\n",
    "AskReddit,\n",
    "todayilearned,\n",
    "worldnews,\n",
    "Science,\n",
    "news,\n",
    "politics,\n",
    "explainlikeimfive,\n",
    "Showerthoughts,\n",
    "funny,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = []\n",
    "my_subreddit = reddit.subreddit('AskReddit')\n",
    "for post in my_subreddit.top(limit=995):\n",
    "    str(datetime.datetime.fromtimestamp(post.created_utc))\n",
    "    posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments,\n",
    "                  str(datetime.datetime.fromtimestamp(post.created))])\n",
    "posts = pd.DataFrame(posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'created'])\n",
    "#print(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataFrame' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-98a7d7b67efd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mposts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'DataFrame' object is not callable"
     ]
    }
   ],
   "source": [
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Alexi\\\\Documents\\\\UofM\\\\Fall2019\\\\ADA\\\\applied-data-analytics\\\\web-scraping'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##change working directory\n",
    "cwd = os.getcwd()\n",
    "os.chdir(\"C:\\\\Users\\\\Alexi\\\\Documents\\\\UofM\\\\Fall2019\\\\ADA\\\\applied-data-analytics\\\\web-scraping\\\\\")\n",
    "cwd #check working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text_Based_Dataset_Project\\posts_reddit_askreddit.txt\n",
      "Text_Based_Dataset_Project\\posts_reddit_explainlikeimfive.txt\n",
      "Text_Based_Dataset_Project\\posts_reddit_funny.txt\n",
      "Text_Based_Dataset_Project\\posts_reddit_news.txt\n",
      "Text_Based_Dataset_Project\\posts_reddit_politics.txt\n",
      "Text_Based_Dataset_Project\\posts_reddit_Science.txt\n",
      "Text_Based_Dataset_Project\\posts_reddit_Showerthoughts.txt\n",
      "Text_Based_Dataset_Project\\posts_reddit_todayilearned.txt\n",
      "Text_Based_Dataset_Project\\posts_reddit_worldnews.txt\n"
     ]
    }
   ],
   "source": [
    "path = \"Text_Based_Dataset_Project/*.txt\"\n",
    "for fname in glob.glob(path):\n",
    "    print(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\x92' in position 4330: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-69db8d176d92>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m            \u001b[1;31m#f is another placeholder to open the file for writing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\x92' in position 4330: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "for fname in glob.glob(path):\n",
    "        #s is just a placeholder variable for the file being read in\n",
    "    s = open(fname, encoding=\"latin-1\").read()\n",
    "    s = s.replace('NULL', '')\n",
    "           #f is another placeholder to open the file for writing\n",
    "    f = open(fname, 'w')\n",
    "    f.write(s)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.to_csv(r'C:\\Users\\Alexi\\Documents\\UofM\\Fall2019\\ADA\\applied-data-analytics\\web-scraping\\posts_reddit_AskReddit.csv',\n",
    "                mode = 'w', index=None, header = True, date_format='%d/%m/%Y', sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
